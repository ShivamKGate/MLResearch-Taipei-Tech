{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8408121,"sourceType":"datasetVersion","datasetId":5003711}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom math import sqrt\nimport csv\n\ndef Get_All_Data(TG,time_lag,TG_in_one_day,forecast_day_number,TG_in_one_week):\n\t#deal with inflow data 处理进站数据\n\tmetro_enter = []\n\twith open('/kaggle/input/dataset2/data/inflowdata/in_'+str(TG)+'min.csv') as f:\n\t\tdata = csv.reader(f, delimiter=\",\")\n\t\tfor line in data:\n\t\t\tline=[int(x) for x in line]\n\t\t\tmetro_enter.append(line)\n\n\tdef get_train_data_enter(data,time_lag,TG_in_one_day,forecast_day_number,TG_in_one_week):\n\t\tdata = np.array(data)\n\t\tdata2 = np.zeros((data.shape[0], data.shape[1]))\n\t\ta = np.max(data)\n\t\tb = np.min(data)\n\t\tfor i in range(len(data)):\n\t\t\tfor j in range(len(data[0])):\n\t\t\t\tdata2[i, j] = round((data[i, j]-b)/(a-b), 5)\n\t\t#不包括第一周和最后一周的数据\n\t\t#not include the first week and the last week among the five weeks\n\t\tX_train_1 = [[] for i in range(TG_in_one_week, len(data2[0]) - time_lag+1 - TG_in_one_day*forecast_day_number)]\n\t\tY_train = []\n\t\tfor index in range(TG_in_one_week, len(data2[0]) - time_lag+1 - TG_in_one_day*forecast_day_number):\n\t\t\tfor i in range(276):\n\t\t\t\ttemp=data2[i,index-TG_in_one_week: index + time_lag-1-TG_in_one_week].tolist()\n\t\t\t\ttemp.extend(data2[i,index-TG_in_one_day: index + time_lag-1-TG_in_one_day])\n\t\t\t\ttemp.extend(data2[i,index: index + time_lag-1])\n\t\t\t\tX_train_1[index-TG_in_one_week].append(temp)\n\t\t\tY_train.append(data2[:,index + time_lag-1])\n\t\tX_train_1,Y_train = np.array(X_train_1), np.array(Y_train)\n\t\tprint(X_train_1.shape,Y_train.shape)\n\n\t\tX_test_1 = [[] for i in range(len(data2[0]) - TG_in_one_day*forecast_day_number,len(data2[0])-time_lag+1)]\n\t\tY_test = []\n\t\tfor index in range(len(data2[0]) - TG_in_one_day*forecast_day_number,len(data2[0])-time_lag+1):\n\t\t\tfor i in range(276):\n\t\t\t\ttemp = data2[i, index-TG_in_one_week: index + time_lag-1-TG_in_one_week].tolist()\n\t\t\t\ttemp.extend(data2[i, index-TG_in_one_day: index + time_lag-1-TG_in_one_day])\n\t\t\t\ttemp.extend(data2[i, index: index + time_lag-1])\n\t\t\t\tX_test_1[index-(len(data2[0]) - TG_in_one_day*forecast_day_number)].append(temp)\n\t\t\tY_test.append(data2[:, index + time_lag-1])\n\t\tX_test_1,Y_test = np.array(X_test_1), np.array(Y_test)\n\t\tprint(X_test_1.shape, Y_test.shape)\n\n\t\tY_test_original = []\n\t\tfor index in range(len(data[0]) - TG_in_one_day*forecast_day_number,len(data[0])-time_lag+1):\n\t\t\tY_test_original.append(data[:, index + time_lag-1])\n\t\tY_test_original = np.array(Y_test_original)\n\n\t\tprint(Y_test_original.shape)\n\n\t\treturn X_train_1,Y_train,X_test_1,Y_test,Y_test_original,a,b\n\n\t#获取训练集和测试集，Y_test_original为没有scale之前的原始测试集，评估精度用，a,b分别为最大值和最小值\n\t#Get the training dataset and the test dataset, Y_test_original is the original test data before scaling, which can be used for evaluation.\n\t#a and b as the maximum and minimum values, respectively.\n\tX_train_1,Y_train,X_test_1,Y_test,Y_test_original,a,b=get_train_data_enter(metro_enter,time_lag,TG_in_one_day,forecast_day_number,TG_in_one_week)\n\tprint(a,b)\n\n\t#deal with outflow data. Similar with the inflow data while not including the testing data for outflow\n\t#处理出站数据\n\tmetro_exit = []\n\twith open('/kaggle/input/dataset2/data/outflowdata/out_'+str(TG)+'min.csv') as f:\n\t\tdata = csv.reader(f, delimiter=\",\")\n\t\tfor line in data:\n\t\t\tline = [int(x) for x in line]\n\t\t\tmetro_exit.append(line)\n\n\tdef get_train_data_exit(data,time_lag,TG_in_one_day,forecast_day_number,TG_in_one_week):\n\t\tdata = np.array(data)\n\t\tdata2 = np.zeros((data.shape[0], data.shape[1]))\n\t\ta = np.max(data)\n\t\tb = np.min(data)\n\t\tfor i in range(len(data)):\n\t\t\tfor j in range(len(data[0])):\n\t\t\t\tdata2[i, j]=round((data[i, j]-b)/(a-b), 5)\n\t\tX_train_1 = [[] for i in range(TG_in_one_week, len(data2[0]) - time_lag+1 - TG_in_one_day*forecast_day_number)]\n\t\tfor index in range(TG_in_one_week, len(data2[0]) - time_lag+1 - TG_in_one_day*forecast_day_number):\n\t\t\tfor i in range(276):\n\t\t\t\ttemp=data2[i, index-TG_in_one_week: index + time_lag-1-TG_in_one_week].tolist()\n\t\t\t\ttemp.extend(data2[i, index-TG_in_one_day: index + time_lag-1-TG_in_one_day])\n\t\t\t\ttemp.extend(data2[i, index: index + time_lag-1])\n\t\t\t\tX_train_1[index-TG_in_one_week].append(temp)\n\t\tX_train_1 = np.array(X_train_1)\n\t\tprint(X_train_1.shape)\n\n\t\tX_test_1 = [[] for i in range(len(data2[0]) - TG_in_one_day*forecast_day_number, len(data2[0])-time_lag+1)]\n\t\tfor index in range(len(data2[0]) - TG_in_one_day*forecast_day_number, len(data2[0])-time_lag+1):\n\t\t\tfor i in range(276):\n\t\t\t\ttemp = data2[i,index-TG_in_one_week: index + time_lag-1-TG_in_one_week].tolist()\n\t\t\t\ttemp.extend(data2[i, index-TG_in_one_day: index + time_lag-1-TG_in_one_day])\n\t\t\t\ttemp.extend(data2[i, index: index + time_lag-1])\n\t\t\t\tX_test_1[index-(len(data2[0]) - TG_in_one_day*forecast_day_number)].append(temp)\n\t\tX_test_1 = np.array(X_test_1)\n\t\tprint(X_test_1.shape)\n\t\treturn X_train_1, X_test_1\n\n\tX_train_2, X_test_2 = get_train_data_exit(metro_exit, time_lag, TG_in_one_day, forecast_day_number, TG_in_one_week)\n\n\t#deal with graph data. involve the adjacency matrix 处理graph图数据，邻接矩阵信息\n\tadjacency = []\n\twith open('/kaggle/input/dataset2/adjacency.csv') as f:\n\t\tdata = csv.reader(f, delimiter=\",\")\n\t\tfor line in data:\n\t\t\tline = [float(x) for x in line]\n\t\t\tadjacency.append(line)\n\tadjacency = np.array(adjacency)\n\t# use adjacency matrix to calculate D_hat**-1/2 * A_hat *D_hat**-1/2\n\tI = np.matrix(np.eye(276))\n\tA_hat = adjacency+I\n\tD_hat = np.array(np.sum(A_hat, axis=0))[0]\n\tD_hat_sqrt = [sqrt(x) for x in D_hat]\n\tD_hat_sqrt = np.array(np.diag(D_hat_sqrt))\n\tD_hat_sqrtm_inv = np.linalg.inv(D_hat_sqrt)# get the D_hat**-1/2 (开方后求逆即为矩阵的-1/2次方)\n\t#D_A_final = D_hat**-1/2 * A_hat *D_hat**-1/2\n\tD_A_final = np.dot(D_hat_sqrtm_inv, A_hat)\n\tD_A_final = np.dot(D_A_final, D_hat_sqrtm_inv)\n\tprint(D_A_final.shape)\n\tdef get_train_data_graph(data,D_A_final,time_lag,TG_in_one_day,forecast_day_number,TG_in_one_week,):\n\t\tdata = np.array(data)\n\t\tdata2 = np.zeros((data.shape[0], data.shape[1]))\n\t\ta = np.max(data)\n\t\tb = np.min(data)\n\t\tfor i in range(len(data)):\n\t\t\tfor j in range(len(data[0])):\n\t\t\t\tdata2[i,j]=round((data[i,j]-b)/(a-b),5)\n\t\tX_train_1 = [[] for i in range(TG_in_one_week, len(data2[0]) - time_lag+1 - TG_in_one_day*forecast_day_number)]\n\t\tfor index in range(TG_in_one_week, len(data2[0]) - time_lag+1 - TG_in_one_day*forecast_day_number):\n\t\t\tfor i in range(276):\n\t\t\t\ttemp=data2[i,index: index + time_lag-1]\n\t\t\t\tX_train_1[index-TG_in_one_week].append(temp)\n\t\t\tX_train_1[index-TG_in_one_week] = np.dot(D_A_final, X_train_1[index-TG_in_one_week])\n\t\tX_train_1= np.array(X_train_1)\n\t\tprint(X_train_1.shape)\n\n\t\tX_test_1 = [[] for i in range(len(data2[0]) - TG_in_one_day*forecast_day_number,len(data2[0])-time_lag+1)]\n\t\tfor index in range(len(data2[0]) - TG_in_one_day*forecast_day_number,len(data2[0])-time_lag+1):\n\t\t\tfor i in range(276):\n\t\t\t\ttemp = data2[i,index: index + time_lag-1]\n\t\t\t\tX_test_1[index-(len(data2[0]) - TG_in_one_day*forecast_day_number)].append(temp)\n\t\t\tX_test_1[index-(len(data2[0]) - TG_in_one_day*forecast_day_number)] = np.dot(D_A_final, X_test_1[index-(len(data2[0]) - TG_in_one_day*forecast_day_number)])\n\t\tX_test_1 = np.array(X_test_1)\n\t\tprint(X_test_1.shape)\n\n\t\treturn X_train_1,X_test_1\n\n\tX_train_3, X_test_3 = get_train_data_graph(metro_enter, D_A_final, time_lag, TG_in_one_day, forecast_day_number, TG_in_one_week)\n\n\t#deal with meteorology data including the weather and PM data 处理11个指标的天气数据\n\tWeather = []\n\twith open('/kaggle/input/dataset2/data/meteorology/'+str(TG)+' min after normolization.csv') as f:\n\t\tdata = csv.reader(f, delimiter=\",\")\n\t\tfor line in data:\n\t\t\tline = [float(x) for x in line]\n\t\t\tWeather.append(line)\n\n\tdef get_train_data_weather_PM(data, time_lag, TG_in_one_day, forecast_day_number, TG_in_one_week,):\n\t\tdata = np.array(data)\n\t\t#不包括第一周和最后一周\n\t\tX_train_1 = [[] for i in range(TG_in_one_week, len(data[0]) - time_lag+1 - TG_in_one_day*forecast_day_number)]\n\t\tfor index in range(TG_in_one_week, len(data[0]) - time_lag+1 - TG_in_one_day*forecast_day_number):\n\t\t\tfor i in range(len(data)):\n\t\t\t\t#For meteorology data，we only consider today's data, namely recent pattern. 天气数据只考虑当天的\n\t\t\t\tX_train_1[index-TG_in_one_week].append(data[i,index: index + time_lag-1])\n\t\tX_train_1 = np.array(X_train_1)\n\t\tprint(X_train_1.shape)\n\n\t\tX_test_1 = [[] for i in range(len(data[0]) - TG_in_one_day*forecast_day_number, len(data[0])-time_lag+1)]\n\t\tfor index in range(len(data[0]) - TG_in_one_day*forecast_day_number, len(data[0])-time_lag+1):\n\t\t\tfor i in range(len(data)):\n\t\t\t\tX_test_1[index-(len(data[0]) - TG_in_one_day*forecast_day_number)].append(data[i, index: index + time_lag-1])\n\t\tX_test_1 = np.array(X_test_1)\n\t\tprint(X_test_1.shape)\n\t\treturn X_train_1,X_test_1\n\n\tX_train_4, X_test_4 = get_train_data_weather_PM(Weather, time_lag, TG_in_one_day, forecast_day_number, TG_in_one_week)\n\n\treturn X_train_1, Y_train, X_test_1, Y_test, Y_test_original, a, b, X_train_2, X_test_2, X_train_3, X_test_3, X_train_4, X_test_4\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom math import sqrt\nimport numpy as np\n\n#define weighted_mean_absolute_percentage_error and other eveluation metrics定义平均绝对百分比误差和评价函数\n# The shape of the two matrixs are all n*276 where 276 is the station numbers 两个矩阵都是n行276列\n\ndef weighted_mean_absolute_percentage_error(Y_true, Y_pred):\n\ttotal_sum=np.sum(Y_true)\n\taverage=[]\n\tfor i in range(len(Y_true)):\n\t\tfor j in range(len(Y_true[0])):\n\t\t\tif Y_true[i][j]>0:\n\t\t\t\t#加权 weighted\n\t\t\t\ttemp=(Y_true[i][j]/total_sum)*np.abs((Y_true[i][j] - Y_pred[i][j]) / Y_true[i][j])\n\t\t\t\taverage.append(temp)\n\treturn np.sum(average)\n\ndef evaluate_performance(Y_test_original,predictions):\n\tRMSE = sqrt(mean_squared_error(Y_test_original, predictions))\n\tprint('RMSE is: '+str(RMSE))\n\tR2 = r2_score(Y_test_original, predictions)\n\tprint(\"R2 is：\"+str(R2))\n\tMAE = mean_absolute_error(Y_test_original, predictions)\n\tprint(\"MAE is：\"+str(MAE))\n\tWMAPE = weighted_mean_absolute_percentage_error(Y_test_original, predictions)\n\tprint(\"WMAPE is: \"+str(WMAPE))\n\treturn RMSE, R2, MAE, WMAPE\n\nfrom numpy.random import seed\nseed(1)\nimport tensorflow as tf\ntf.random.set_seed(2)\nimport numpy as np\nnp.set_printoptions(threshold=np.inf)\nimport time\nimport os\n\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\ntf.keras.backend.set_image_data_format('channels_last')\n\n# Import layers, models, and utilities\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\n\n\n# os.chdir('D:/论文2/upload to GitHub/')\n#os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin' #used for visualizing the model\n\nglobal_start_time = time.time()\n\ndef Unit(x, filters, pool=False):\n\tres = x\n\tif pool:\n\t\tx = AveragePooling2D(pool_size=(2, 2), padding=\"same\")(x)\n\t\tres = Conv2D(filters=filters, kernel_size=[1, 1], strides=(2, 2), padding=\"same\")(res)\n\tout = BatchNormalization()(x)\n\tout = Activation(\"relu\")(out)\n\tout = Conv2D(filters=filters, kernel_size=[3, 3], strides=[1, 1], padding=\"same\")(out)\n\n\tout = BatchNormalization()(out)\n\tout = Activation(\"relu\")(out)\n\tout = Conv2D(filters=filters, kernel_size=[3, 3], strides=[1, 1], padding=\"same\")(out)\n\n\tout = tf.keras.layers.add([res, out])\n\n\treturn out\n\ndef attention_3d_block(inputs,timesteps): #input_dim = int(inputs.shape[2])\n    a = Permute((2, 1))(inputs)\n    a = Dense(timesteps, activation='linear')(a)\n    a_probs = Permute((2, 1))(a)\n    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n    output_attention_mul = multiply([inputs, a_probs])\n    return output_attention_mul\n\n# Define the model\ndef multi_input_model(time_lag):\n    input1_ = Input(shape=(276, time_lag-1, 3), name='input1')\n    input2_ = Input(shape=(276, time_lag-1, 3), name='input2')\n    input3_ = Input(shape=(276, time_lag-1, 1), name='input3')\n    input4_ = Input(shape=(11, time_lag-1, 1), name='input4')\n    #first input\n    x1 = Conv2D(filters=32, kernel_size=[3, 3], strides=[1, 1], padding=\"same\")(input1_)\n    x1 = Unit(x1, 32)\n    x1 = Unit(x1, 64, pool=True)\n    x1 = Flatten()(x1)\n    x1 = Dense(276)(x1)\n\n    # second input\n    x2 = Conv2D(filters=32, kernel_size=[3, 3], strides=[1, 1], padding=\"same\")(input2_)\n    x2 = Unit(x2, 32)\n    x2 = Unit(x2, 64, pool=True)\n    x2 = Flatten()(x2)\n    x2 = Dense(276)(x2)\n\n    # third input\n    x3 = Conv2D(filters=32, kernel_size=[3, 3], strides=[1, 1], padding=\"same\")(input3_)\n    x3 = Unit(x3, 32)\n    x3 = Unit(x3, 64, pool=True)\n    x3 = Flatten()(x3)\n    x3 = Dense(276)(x3)\n\n    # fourth input\n    x4 = Flatten()(input4_)\n    x4 = Dense(276)(x4)\n    x4 = Reshape(target_shape=(276, 1))(x4)\n    x4 = LSTM(128, return_sequences=True, input_shape=(276, 1))(x4)\n    x4 = LSTM(276, return_sequences=False)(x4)\n    x4 = Dense(276)(x4)\n\n    out = add([x1, x2, x3, x4])\n    out = Reshape(target_shape=(276, 1))(out)\n    out = LSTM(128, return_sequences=True,input_shape=(276, 1))(out)\n    out = attention_3d_block(out, 276)#shape of the output is（276，128）\n    out = Flatten()(out)\n    out = Dense(276)(out)\n\n    model = Model(inputs=[input1_, input2_, input3_,input4_], outputs=[out]) #[input1_, input2_, input3_]\n    return model\n\ndef build_model(X_train_1,X_train_2,X_train_3,X_train_4,Y_train,X_test_1,X_test_2,X_test_3,X_test_4,Y_test, Y_test_original,batch_size,epochs,a,time_lag):\n\tX_train_1 = X_train_1.reshape(X_train_1.shape[0],  276, time_lag-1, 3)\n\tX_train_2 = X_train_2.reshape(X_train_2.shape[0],  276, time_lag-1, 3)\n\tX_train_3 = X_train_3.reshape(X_train_3.shape[0],  276, time_lag-1, 1)\n\tX_train_4 = X_train_4.reshape(X_train_4.shape[0],  11, time_lag-1, 1)\n\tY_train = Y_train.reshape(Y_train.shape[0], 276)\n\n\tX_test_1 = X_test_1.reshape(X_test_1.shape[0],  276, time_lag-1, 3)\n\tX_test_2 = X_test_2.reshape(X_test_2.shape[0],  276, time_lag-1, 3)\n\tX_test_3 = X_test_3.reshape(X_test_3.shape[0],  276, time_lag-1, 1)\n\tX_test_4 = X_test_4.reshape(X_test_4.shape[0],  11, time_lag-1, 1)\n\tY_test = Y_test.reshape(Y_test.shape[0], 276)\n\n\tif epochs == 50:\n\t\tmodel = multi_input_model(time_lag)\n\t\tmodel.compile(optimizer=Adam(), loss='mse', metrics=['mse'])\n\t\tmodel.fit([X_train_1, X_train_2, X_train_3, X_train_4], Y_train, batch_size=batch_size, epochs=epochs, verbose=2, shuffle=False)#, validation_split=0.05\n\t\toutput = model.predict([X_test_1, X_test_2, X_test_3, X_test_4], batch_size=batch_size)\n\telse:\n\t\t# Load the previously saved model and continue training\n\t\tmodel_path = f'/kaggle/working/testresult/{epochs - 10}/10-model-with-graph.keras'  \n\t\tif os.path.exists(model_path):\n\t\t\tmodel = load_model(model_path)\n\t\t\tmodel.fit([X_train_1, X_train_2, X_train_3, X_train_4], Y_train, batch_size=batch_size, epochs=10,\n\t\t\t\t\t  verbose=2, shuffle=False)\n\t\t\toutput = model.predict([X_test_1, X_test_2, X_test_3, X_test_4], batch_size=batch_size)\n\t\telse:\n\t\t\traise FileNotFoundError(f\"Model file not found: {model_path}\")\n    \n    \n\t#rescale the output of this model将输出进行反归一化\n\tpredictions = np.zeros((output.shape[0], output.shape[1]))\n\tfor i in range(len(predictions)):\n\t\tfor j in range(len(predictions[0])):\n\t\t\tpredictions[i, j] = round(output[i, j]*a, 0)\n\t\t\tif predictions[i, j] < 0:\n\t\t\t\tpredictions[i, j] = 0\n\n\tRMSE,R2,MAE,WMAPE=evaluate_performance(Y_test_original,predictions)\n\t#visualize the model structure\n\t#print(model.summary())\n\tplot_model(model, to_file='model.png', show_shapes=True)\n\n\treturn model,Y_test_original,predictions,RMSE,R2,MAE,WMAPE\n\nimport os\nimport time\nimport numpy as np\n\ndef Save_Data(path, model, Y_test_original, predictions, RMSE, R2, MAE, WMAPE, Run_epoch, global_start_time):\n    directory = os.path.join(path, str(Run_epoch))\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n        \n    model.save(os.path.join(directory, '10-model-with-graph.keras'))\n    np.savetxt(os.path.join(directory, '10-RMSE_ALL.txt'), [RMSE])\n    np.savetxt(os.path.join(directory, '10-R2_ALL.txt'), [R2])\n    np.savetxt(os.path.join(directory, '10-MAE_ALL.txt'), [MAE])\n    np.savetxt(os.path.join(directory, '10-WMAPE_ALL.txt'), [WMAPE])\n    np.savetxt(os.path.join(directory, '10-predictions.csv'), predictions, delimiter=\",\")\n    np.savetxt(os.path.join(directory, '10-Y_test_original.csv'), Y_test_original, delimiter=\",\")\n    \n    duration_time = time.time() - global_start_time\n    np.savetxt(os.path.join(directory, '10-Average_train_time.txt'), [duration_time])\n    \n    print('total training time(s):', duration_time)\n\n# Define the path to save the results\noutput_path = \"/kaggle/working/testresult/\"\n\n# Assuming Get_All_Data and build_model functions are defined elsewhere\nX_train_1, Y_train, X_test_1, Y_test, Y_test_original, a, b, X_train_2, X_test_2, X_train_3, X_test_3, X_train_4, X_test_4 = Get_All_Data(TG=15, time_lag=6, TG_in_one_day=72, forecast_day_number=5, TG_in_one_week=360)\n\nRun_epoch = 50  # first training 50 epoch, and then add 10 epoch every time 初始训练epoch，以后每次加10，运行15次\nglobal_start_time = time.time()\n\nfor i in range(15):\n    model, Y_test_original, predictions, RMSE, R2, MAE, WMAPE = build_model(X_train_1, X_train_2, X_train_3, X_train_4, Y_train, X_test_1, X_test_2, X_test_3, X_test_4, Y_test,Y_test_original, batch_size=64, epochs=Run_epoch, a=a, time_lag=6)\n    Save_Data(output_path, model, Y_test_original, predictions, RMSE, R2, MAE, WMAPE, Run_epoch, global_start_time)\n    Run_epoch += 10\n\n\n#For Get_All_Data, change parameters referring to this: TG=15, time_lag=6, TG_in_one_day=72, forecast_day_number=5, TG_in_one_week=360\n#10min:10,6,108,5,540,eopch=200\n#15min:15,6,72,5,360 eopch=140\n#30min:30,6,36,5,180 eopch=200\n#60min:60,6,18,5,90 eopch=235","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:51:48.706999Z","iopub.execute_input":"2024-05-24T11:51:48.707704Z","iopub.status.idle":"2024-05-24T12:04:10.214396Z","shell.execute_reply.started":"2024-05-24T11:51:48.707675Z","shell.execute_reply":"2024-05-24T12:04:10.213440Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"(1075, 276, 15) (1075, 276)\n(355, 276, 15) (355, 276)\n(355, 276)\n4744 0\n(1075, 276, 15)\n(355, 276, 15)\n(276, 276)\n(1075, 276, 5)\n(355, 276, 5)\n(1075, 11, 5)\n(355, 11, 5)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n17/17 - 14s - 852ms/step - loss: 0.0249 - mse: 0.0249\nEpoch 2/50\n17/17 - 2s - 113ms/step - loss: 0.0026 - mse: 0.0026\nEpoch 3/50\n17/17 - 2s - 113ms/step - loss: 9.6140e-04 - mse: 9.6140e-04\nEpoch 4/50\n17/17 - 2s - 113ms/step - loss: 3.9508e-04 - mse: 3.9508e-04\nEpoch 5/50\n17/17 - 2s - 112ms/step - loss: 2.5438e-04 - mse: 2.5438e-04\nEpoch 6/50\n17/17 - 2s - 112ms/step - loss: 2.4075e-04 - mse: 2.4075e-04\nEpoch 7/50\n17/17 - 2s - 113ms/step - loss: 2.3024e-04 - mse: 2.3024e-04\nEpoch 8/50\n17/17 - 2s - 112ms/step - loss: 1.9885e-04 - mse: 1.9885e-04\nEpoch 9/50\n17/17 - 2s - 113ms/step - loss: 1.5861e-04 - mse: 1.5861e-04\nEpoch 10/50\n17/17 - 2s - 113ms/step - loss: 1.4207e-04 - mse: 1.4207e-04\nEpoch 11/50\n17/17 - 2s - 112ms/step - loss: 1.3970e-04 - mse: 1.3970e-04\nEpoch 12/50\n17/17 - 2s - 113ms/step - loss: 1.2999e-04 - mse: 1.2999e-04\nEpoch 13/50\n17/17 - 2s - 112ms/step - loss: 1.2904e-04 - mse: 1.2904e-04\nEpoch 14/50\n17/17 - 2s - 112ms/step - loss: 1.2669e-04 - mse: 1.2669e-04\nEpoch 15/50\n17/17 - 2s - 112ms/step - loss: 1.1906e-04 - mse: 1.1906e-04\nEpoch 16/50\n17/17 - 2s - 112ms/step - loss: 1.1784e-04 - mse: 1.1784e-04\nEpoch 17/50\n17/17 - 2s - 113ms/step - loss: 1.1698e-04 - mse: 1.1698e-04\nEpoch 18/50\n17/17 - 2s - 113ms/step - loss: 1.1645e-04 - mse: 1.1645e-04\nEpoch 19/50\n17/17 - 2s - 113ms/step - loss: 1.1840e-04 - mse: 1.1840e-04\nEpoch 20/50\n17/17 - 2s - 113ms/step - loss: 1.2543e-04 - mse: 1.2543e-04\nEpoch 21/50\n17/17 - 2s - 113ms/step - loss: 1.2736e-04 - mse: 1.2736e-04\nEpoch 22/50\n17/17 - 2s - 113ms/step - loss: 1.3808e-04 - mse: 1.3808e-04\nEpoch 23/50\n17/17 - 2s - 113ms/step - loss: 1.3852e-04 - mse: 1.3852e-04\nEpoch 24/50\n17/17 - 2s - 112ms/step - loss: 1.2549e-04 - mse: 1.2549e-04\nEpoch 25/50\n17/17 - 2s - 113ms/step - loss: 1.2646e-04 - mse: 1.2646e-04\nEpoch 26/50\n17/17 - 2s - 113ms/step - loss: 1.3103e-04 - mse: 1.3103e-04\nEpoch 27/50\n17/17 - 2s - 112ms/step - loss: 1.4663e-04 - mse: 1.4663e-04\nEpoch 28/50\n17/17 - 2s - 113ms/step - loss: 1.6232e-04 - mse: 1.6232e-04\nEpoch 29/50\n17/17 - 2s - 113ms/step - loss: 1.3622e-04 - mse: 1.3622e-04\nEpoch 30/50\n17/17 - 2s - 112ms/step - loss: 1.1891e-04 - mse: 1.1891e-04\nEpoch 31/50\n17/17 - 2s - 113ms/step - loss: 1.0080e-04 - mse: 1.0080e-04\nEpoch 32/50\n17/17 - 2s - 113ms/step - loss: 8.7364e-05 - mse: 8.7364e-05\nEpoch 33/50\n17/17 - 2s - 113ms/step - loss: 8.4348e-05 - mse: 8.4348e-05\nEpoch 34/50\n17/17 - 2s - 113ms/step - loss: 8.3682e-05 - mse: 8.3682e-05\nEpoch 35/50\n17/17 - 2s - 113ms/step - loss: 8.0574e-05 - mse: 8.0574e-05\nEpoch 36/50\n17/17 - 2s - 113ms/step - loss: 7.7352e-05 - mse: 7.7352e-05\nEpoch 37/50\n17/17 - 2s - 113ms/step - loss: 7.5530e-05 - mse: 7.5530e-05\nEpoch 38/50\n17/17 - 2s - 112ms/step - loss: 7.3630e-05 - mse: 7.3630e-05\nEpoch 39/50\n17/17 - 2s - 112ms/step - loss: 7.1637e-05 - mse: 7.1637e-05\nEpoch 40/50\n17/17 - 2s - 113ms/step - loss: 7.1246e-05 - mse: 7.1246e-05\nEpoch 41/50\n17/17 - 2s - 112ms/step - loss: 7.2298e-05 - mse: 7.2298e-05\nEpoch 42/50\n17/17 - 2s - 113ms/step - loss: 7.4186e-05 - mse: 7.4186e-05\nEpoch 43/50\n17/17 - 2s - 112ms/step - loss: 7.5418e-05 - mse: 7.5418e-05\nEpoch 44/50\n17/17 - 2s - 112ms/step - loss: 7.7103e-05 - mse: 7.7103e-05\nEpoch 45/50\n17/17 - 2s - 113ms/step - loss: 8.0542e-05 - mse: 8.0542e-05\nEpoch 46/50\n17/17 - 2s - 113ms/step - loss: 8.9748e-05 - mse: 8.9748e-05\nEpoch 47/50\n17/17 - 2s - 113ms/step - loss: 1.0544e-04 - mse: 1.0544e-04\nEpoch 48/50\n17/17 - 2s - 113ms/step - loss: 1.1084e-04 - mse: 1.1084e-04\nEpoch 49/50\n17/17 - 2s - 112ms/step - loss: 9.2117e-05 - mse: 9.2117e-05\nEpoch 50/50\n17/17 - 2s - 113ms/step - loss: 7.8387e-05 - mse: 7.8387e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 167ms/step\nRMSE is: 49.510760457725404\nR2 is：0.9183674739086942\nMAE is：29.179230455194936\nWMAPE is: 0.10877977644696295\ntotal training time(s): 116.36600470542908\nEpoch 1/10\n17/17 - 11s - 662ms/step - loss: 7.5971e-05 - mse: 7.5971e-05\nEpoch 2/10\n17/17 - 2s - 112ms/step - loss: 6.9890e-05 - mse: 6.9890e-05\nEpoch 3/10\n17/17 - 2s - 112ms/step - loss: 6.5786e-05 - mse: 6.5786e-05\nEpoch 4/10\n17/17 - 2s - 112ms/step - loss: 6.6005e-05 - mse: 6.6005e-05\nEpoch 5/10\n17/17 - 2s - 111ms/step - loss: 6.3504e-05 - mse: 6.3504e-05\nEpoch 6/10\n17/17 - 2s - 112ms/step - loss: 6.3533e-05 - mse: 6.3533e-05\nEpoch 7/10\n17/17 - 3s - 149ms/step - loss: 6.2818e-05 - mse: 6.2818e-05\nEpoch 8/10\n17/17 - 2s - 112ms/step - loss: 6.1028e-05 - mse: 6.1028e-05\nEpoch 9/10\n17/17 - 2s - 112ms/step - loss: 5.9700e-05 - mse: 5.9700e-05\nEpoch 10/10\n17/17 - 2s - 112ms/step - loss: 5.9878e-05 - mse: 5.9878e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 168ms/step\nRMSE is: 42.83670687398026\nR2 is：0.949169338541126\nMAE is：24.912135129618285\nWMAPE is: 0.09307132094002449\ntotal training time(s): 159.23802423477173\nEpoch 1/10\n17/17 - 13s - 775ms/step - loss: 6.1489e-05 - mse: 6.1489e-05\nEpoch 2/10\n17/17 - 2s - 113ms/step - loss: 6.3529e-05 - mse: 6.3529e-05\nEpoch 3/10\n17/17 - 2s - 113ms/step - loss: 6.4271e-05 - mse: 6.4271e-05\nEpoch 4/10\n17/17 - 2s - 113ms/step - loss: 6.3935e-05 - mse: 6.3935e-05\nEpoch 5/10\n17/17 - 2s - 113ms/step - loss: 6.5486e-05 - mse: 6.5486e-05\nEpoch 6/10\n17/17 - 2s - 113ms/step - loss: 6.9688e-05 - mse: 6.9688e-05\nEpoch 7/10\n17/17 - 2s - 112ms/step - loss: 7.5888e-05 - mse: 7.5888e-05\nEpoch 8/10\n17/17 - 2s - 112ms/step - loss: 7.6570e-05 - mse: 7.6570e-05\nEpoch 9/10\n17/17 - 2s - 113ms/step - loss: 7.2343e-05 - mse: 7.2343e-05\nEpoch 10/10\n17/17 - 2s - 113ms/step - loss: 7.1917e-05 - mse: 7.1917e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 184ms/step\nRMSE is: 46.59462340650429\nR2 is：0.9344097217439407\nMAE is：27.4194325372525\nWMAPE is: 0.10249185980839755\ntotal training time(s): 203.60885286331177\nEpoch 1/10\n17/17 - 11s - 674ms/step - loss: 7.2485e-05 - mse: 7.2485e-05\nEpoch 2/10\n17/17 - 2s - 112ms/step - loss: 7.2963e-05 - mse: 7.2963e-05\nEpoch 3/10\n17/17 - 2s - 112ms/step - loss: 6.9979e-05 - mse: 6.9979e-05\nEpoch 4/10\n17/17 - 2s - 112ms/step - loss: 6.8517e-05 - mse: 6.8517e-05\nEpoch 5/10\n17/17 - 3s - 150ms/step - loss: 6.8043e-05 - mse: 6.8043e-05\nEpoch 6/10\n17/17 - 2s - 112ms/step - loss: 6.8099e-05 - mse: 6.8099e-05\nEpoch 7/10\n17/17 - 2s - 112ms/step - loss: 6.4997e-05 - mse: 6.4997e-05\nEpoch 8/10\n17/17 - 2s - 112ms/step - loss: 6.3934e-05 - mse: 6.3934e-05\nEpoch 9/10\n17/17 - 2s - 112ms/step - loss: 6.4554e-05 - mse: 6.4554e-05\nEpoch 10/10\n17/17 - 2s - 112ms/step - loss: 6.4772e-05 - mse: 6.4772e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 170ms/step\nRMSE is: 43.2874153374542\nR2 is：0.9380592628374765\nMAE is：25.705368442539292\nWMAPE is: 0.09604344096539283\ntotal training time(s): 247.08529806137085\nEpoch 1/10\n17/17 - 12s - 682ms/step - loss: 6.5896e-05 - mse: 6.5896e-05\nEpoch 2/10\n17/17 - 2s - 112ms/step - loss: 6.3590e-05 - mse: 6.3590e-05\nEpoch 3/10\n17/17 - 2s - 112ms/step - loss: 6.0682e-05 - mse: 6.0682e-05\nEpoch 4/10\n17/17 - 2s - 112ms/step - loss: 6.0250e-05 - mse: 6.0250e-05\nEpoch 5/10\n17/17 - 2s - 112ms/step - loss: 6.1458e-05 - mse: 6.1458e-05\nEpoch 6/10\n17/17 - 2s - 112ms/step - loss: 6.4949e-05 - mse: 6.4949e-05\nEpoch 7/10\n17/17 - 2s - 112ms/step - loss: 6.7904e-05 - mse: 6.7904e-05\nEpoch 8/10\n17/17 - 2s - 112ms/step - loss: 6.4804e-05 - mse: 6.4804e-05\nEpoch 9/10\n17/17 - 2s - 112ms/step - loss: 6.6845e-05 - mse: 6.6845e-05\nEpoch 10/10\n17/17 - 2s - 112ms/step - loss: 6.3601e-05 - mse: 6.3601e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 170ms/step\nRMSE is: 43.11417088342884\nR2 is：0.9433380821166663\nMAE is：26.232629107981225\nWMAPE is: 0.09799516000593989\ntotal training time(s): 289.94691491127014\nEpoch 1/10\n17/17 - 12s - 686ms/step - loss: 6.2652e-05 - mse: 6.2652e-05\nEpoch 2/10\n17/17 - 2s - 112ms/step - loss: 6.1260e-05 - mse: 6.1260e-05\nEpoch 3/10\n17/17 - 2s - 112ms/step - loss: 6.4593e-05 - mse: 6.4593e-05\nEpoch 4/10\n17/17 - 2s - 112ms/step - loss: 7.0627e-05 - mse: 7.0627e-05\nEpoch 5/10\n17/17 - 2s - 112ms/step - loss: 6.7091e-05 - mse: 6.7091e-05\nEpoch 6/10\n17/17 - 2s - 112ms/step - loss: 6.7127e-05 - mse: 6.7127e-05\nEpoch 7/10\n17/17 - 2s - 112ms/step - loss: 6.4591e-05 - mse: 6.4591e-05\nEpoch 8/10\n17/17 - 2s - 112ms/step - loss: 6.5777e-05 - mse: 6.5777e-05\nEpoch 9/10\n17/17 - 2s - 112ms/step - loss: 6.4394e-05 - mse: 6.4394e-05\nEpoch 10/10\n17/17 - 2s - 112ms/step - loss: 6.4619e-05 - mse: 6.4619e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 172ms/step\nRMSE is: 45.246190882158956\nR2 is：0.9438045799316787\nMAE is：27.26712594407022\nWMAPE is: 0.1019331607342024\ntotal training time(s): 333.19986295700073\nEpoch 1/10\n17/17 - 14s - 796ms/step - loss: 6.3667e-05 - mse: 6.3667e-05\nEpoch 2/10\n17/17 - 2s - 112ms/step - loss: 6.8142e-05 - mse: 6.8142e-05\nEpoch 3/10\n17/17 - 2s - 112ms/step - loss: 7.3073e-05 - mse: 7.3073e-05\nEpoch 4/10\n17/17 - 2s - 112ms/step - loss: 6.8955e-05 - mse: 6.8955e-05\nEpoch 5/10\n17/17 - 2s - 112ms/step - loss: 6.4054e-05 - mse: 6.4054e-05\nEpoch 6/10\n17/17 - 2s - 112ms/step - loss: 6.5410e-05 - mse: 6.5410e-05\nEpoch 7/10\n17/17 - 2s - 112ms/step - loss: 6.7320e-05 - mse: 6.7320e-05\nEpoch 8/10\n17/17 - 2s - 113ms/step - loss: 7.2888e-05 - mse: 7.2888e-05\nEpoch 9/10\n17/17 - 2s - 112ms/step - loss: 7.5316e-05 - mse: 7.5316e-05\nEpoch 10/10\n17/17 - 2s - 112ms/step - loss: 7.9054e-05 - mse: 7.9054e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 171ms/step\nRMSE is: 52.04277479388067\nR2 is：0.9188819554972145\nMAE is：31.749040620534803\nWMAPE is: 0.1184283370274074\ntotal training time(s): 377.658203125\nEpoch 1/10\n17/17 - 11s - 676ms/step - loss: 8.5280e-05 - mse: 8.5280e-05\nEpoch 2/10\n17/17 - 2s - 113ms/step - loss: 7.2367e-05 - mse: 7.2367e-05\nEpoch 3/10\n17/17 - 2s - 112ms/step - loss: 8.2249e-05 - mse: 8.2249e-05\nEpoch 4/10\n17/17 - 2s - 112ms/step - loss: 7.6498e-05 - mse: 7.6498e-05\nEpoch 5/10\n17/17 - 2s - 112ms/step - loss: 8.4842e-05 - mse: 8.4842e-05\nEpoch 6/10\n17/17 - 2s - 113ms/step - loss: 7.4369e-05 - mse: 7.4369e-05\nEpoch 7/10\n17/17 - 2s - 113ms/step - loss: 6.1266e-05 - mse: 6.1266e-05\nEpoch 8/10\n17/17 - 2s - 112ms/step - loss: 5.9690e-05 - mse: 5.9690e-05\nEpoch 9/10\n17/17 - 2s - 113ms/step - loss: 7.5309e-05 - mse: 7.5309e-05\nEpoch 10/10\n17/17 - 2s - 112ms/step - loss: 6.4769e-05 - mse: 6.4769e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 169ms/step\nRMSE is: 49.99931791737254\nR2 is：0.9388823293998388\nMAE is：29.28966115533783\nWMAPE is: 0.10945340396959652\ntotal training time(s): 420.23667669296265\nEpoch 1/10\n17/17 - 12s - 685ms/step - loss: 6.3718e-05 - mse: 6.3718e-05\nEpoch 2/10\n17/17 - 2s - 112ms/step - loss: 8.0820e-05 - mse: 8.0820e-05\nEpoch 3/10\n17/17 - 2s - 112ms/step - loss: 8.6685e-05 - mse: 8.6685e-05\nEpoch 4/10\n17/17 - 2s - 112ms/step - loss: 8.2553e-05 - mse: 8.2553e-05\nEpoch 5/10\n17/17 - 2s - 113ms/step - loss: 8.0743e-05 - mse: 8.0743e-05\nEpoch 6/10\n17/17 - 2s - 113ms/step - loss: 7.8519e-05 - mse: 7.8519e-05\nEpoch 7/10\n17/17 - 2s - 113ms/step - loss: 7.2070e-05 - mse: 7.2070e-05\nEpoch 8/10\n17/17 - 2s - 112ms/step - loss: 6.7828e-05 - mse: 6.7828e-05\nEpoch 9/10\n17/17 - 2s - 113ms/step - loss: 6.7939e-05 - mse: 6.7939e-05\nEpoch 10/10\n17/17 - 2s - 113ms/step - loss: 6.4192e-05 - mse: 6.4192e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 168ms/step\nRMSE is: 45.30750922216667\nR2 is：0.9358404116800635\nMAE is：26.576107368850785\nWMAPE is: 0.09912024299070281\ntotal training time(s): 462.6694655418396\nEpoch 1/10\n17/17 - 12s - 687ms/step - loss: 6.3897e-05 - mse: 6.3897e-05\nEpoch 2/10\n17/17 - 2s - 112ms/step - loss: 6.9321e-05 - mse: 6.9321e-05\nEpoch 3/10\n17/17 - 2s - 112ms/step - loss: 6.6927e-05 - mse: 6.6927e-05\nEpoch 4/10\n17/17 - 2s - 112ms/step - loss: 6.4293e-05 - mse: 6.4293e-05\nEpoch 5/10\n17/17 - 2s - 112ms/step - loss: 5.9239e-05 - mse: 5.9239e-05\nEpoch 6/10\n17/17 - 2s - 112ms/step - loss: 5.2961e-05 - mse: 5.2961e-05\nEpoch 7/10\n17/17 - 2s - 113ms/step - loss: 5.1201e-05 - mse: 5.1201e-05\nEpoch 8/10\n17/17 - 2s - 112ms/step - loss: 4.5692e-05 - mse: 4.5692e-05\nEpoch 9/10\n17/17 - 2s - 111ms/step - loss: 4.1293e-05 - mse: 4.1293e-05\nEpoch 10/10\n17/17 - 2s - 111ms/step - loss: 4.1958e-05 - mse: 4.1958e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 174ms/step\nRMSE is: 40.741028388616726\nR2 is：0.958704640242757\nMAE is：23.505756276791182\nWMAPE is: 0.08783599748640855\ntotal training time(s): 505.04581594467163\nEpoch 1/10\n17/17 - 12s - 685ms/step - loss: 4.1128e-05 - mse: 4.1128e-05\nEpoch 2/10\n17/17 - 2s - 112ms/step - loss: 4.2041e-05 - mse: 4.2041e-05\nEpoch 3/10\n17/17 - 2s - 113ms/step - loss: 4.2600e-05 - mse: 4.2600e-05\nEpoch 4/10\n17/17 - 2s - 113ms/step - loss: 4.0801e-05 - mse: 4.0801e-05\nEpoch 5/10\n17/17 - 2s - 112ms/step - loss: 4.1941e-05 - mse: 4.1941e-05\nEpoch 6/10\n17/17 - 2s - 113ms/step - loss: 4.4401e-05 - mse: 4.4401e-05\nEpoch 7/10\n17/17 - 2s - 113ms/step - loss: 4.5402e-05 - mse: 4.5402e-05\nEpoch 8/10\n17/17 - 2s - 114ms/step - loss: 4.9878e-05 - mse: 4.9878e-05\nEpoch 9/10\n17/17 - 2s - 113ms/step - loss: 5.0891e-05 - mse: 5.0891e-05\nEpoch 10/10\n17/17 - 2s - 113ms/step - loss: 5.4022e-05 - mse: 5.4022e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 613ms/step\nRMSE is: 50.07777794233246\nR2 is：0.9392296020425672\nMAE is：29.23232292304552\nWMAPE is: 0.10935097771764488\ntotal training time(s): 549.8782076835632\nEpoch 1/10\n17/17 - 12s - 692ms/step - loss: 5.4293e-05 - mse: 5.4293e-05\nEpoch 2/10\n17/17 - 2s - 112ms/step - loss: 5.3493e-05 - mse: 5.3493e-05\nEpoch 3/10\n17/17 - 2s - 113ms/step - loss: 4.5994e-05 - mse: 4.5994e-05\nEpoch 4/10\n17/17 - 2s - 112ms/step - loss: 4.9846e-05 - mse: 4.9846e-05\nEpoch 5/10\n17/17 - 2s - 113ms/step - loss: 4.3517e-05 - mse: 4.3517e-05\nEpoch 6/10\n17/17 - 2s - 112ms/step - loss: 4.5598e-05 - mse: 4.5598e-05\nEpoch 7/10\n17/17 - 2s - 112ms/step - loss: 4.6768e-05 - mse: 4.6768e-05\nEpoch 8/10\n17/17 - 2s - 112ms/step - loss: 4.8561e-05 - mse: 4.8561e-05\nEpoch 9/10\n17/17 - 2s - 113ms/step - loss: 4.4949e-05 - mse: 4.4949e-05\nEpoch 10/10\n17/17 - 2s - 112ms/step - loss: 4.5931e-05 - mse: 4.5931e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 180ms/step\nRMSE is: 43.063904459766086\nR2 is：0.9508339021407775\nMAE is：24.62027964890794\nWMAPE is: 0.09211771303551648\ntotal training time(s): 592.6769490242004\nEpoch 1/10\n17/17 - 12s - 688ms/step - loss: 4.6115e-05 - mse: 4.6115e-05\nEpoch 2/10\n17/17 - 2s - 113ms/step - loss: 3.6313e-05 - mse: 3.6313e-05\nEpoch 3/10\n17/17 - 2s - 113ms/step - loss: 3.4841e-05 - mse: 3.4841e-05\nEpoch 4/10\n17/17 - 3s - 149ms/step - loss: 3.4024e-05 - mse: 3.4024e-05\nEpoch 5/10\n17/17 - 2s - 113ms/step - loss: 3.4539e-05 - mse: 3.4539e-05\nEpoch 6/10\n17/17 - 2s - 113ms/step - loss: 3.6575e-05 - mse: 3.6575e-05\nEpoch 7/10\n17/17 - 2s - 113ms/step - loss: 3.6791e-05 - mse: 3.6791e-05\nEpoch 8/10\n17/17 - 2s - 113ms/step - loss: 3.7609e-05 - mse: 3.7609e-05\nEpoch 9/10\n17/17 - 2s - 113ms/step - loss: 3.7651e-05 - mse: 3.7651e-05\nEpoch 10/10\n17/17 - 2s - 112ms/step - loss: 3.9667e-05 - mse: 3.9667e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 172ms/step\nRMSE is: 41.434351334420384\nR2 is：0.9480373269069836\nMAE is：24.435497040212287\nWMAPE is: 0.09142309558667111\ntotal training time(s): 635.6874208450317\nEpoch 1/10\n17/17 - 12s - 690ms/step - loss: 4.0322e-05 - mse: 4.0322e-05\nEpoch 2/10\n17/17 - 2s - 113ms/step - loss: 4.1943e-05 - mse: 4.1943e-05\nEpoch 3/10\n17/17 - 2s - 112ms/step - loss: 4.2450e-05 - mse: 4.2450e-05\nEpoch 4/10\n17/17 - 2s - 112ms/step - loss: 4.1168e-05 - mse: 4.1168e-05\nEpoch 5/10\n17/17 - 2s - 112ms/step - loss: 3.9694e-05 - mse: 3.9694e-05\nEpoch 6/10\n17/17 - 2s - 112ms/step - loss: 3.9386e-05 - mse: 3.9386e-05\nEpoch 7/10\n17/17 - 2s - 112ms/step - loss: 3.9228e-05 - mse: 3.9228e-05\nEpoch 8/10\n17/17 - 2s - 112ms/step - loss: 3.7339e-05 - mse: 3.7339e-05\nEpoch 9/10\n17/17 - 2s - 112ms/step - loss: 3.5879e-05 - mse: 3.5879e-05\nEpoch 10/10\n17/17 - 2s - 112ms/step - loss: 3.7664e-05 - mse: 3.7664e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 169ms/step\nRMSE is: 39.7516700849892\nR2 is：0.9610150079351769\nMAE is：23.07218820167381\nWMAPE is: 0.08600773285704169\ntotal training time(s): 678.052768945694\nEpoch 1/10\n17/17 - 12s - 678ms/step - loss: 3.8950e-05 - mse: 3.8950e-05\nEpoch 2/10\n17/17 - 2s - 112ms/step - loss: 3.7950e-05 - mse: 3.7950e-05\nEpoch 3/10\n17/17 - 2s - 112ms/step - loss: 3.9929e-05 - mse: 3.9929e-05\nEpoch 4/10\n17/17 - 2s - 112ms/step - loss: 3.9280e-05 - mse: 3.9280e-05\nEpoch 5/10\n17/17 - 2s - 112ms/step - loss: 3.8805e-05 - mse: 3.8805e-05\nEpoch 6/10\n17/17 - 2s - 112ms/step - loss: 3.6229e-05 - mse: 3.6229e-05\nEpoch 7/10\n17/17 - 2s - 112ms/step - loss: 3.6211e-05 - mse: 3.6211e-05\nEpoch 8/10\n17/17 - 2s - 112ms/step - loss: 3.7779e-05 - mse: 3.7779e-05\nEpoch 9/10\n17/17 - 2s - 112ms/step - loss: 4.2730e-05 - mse: 4.2730e-05\nEpoch 10/10\n17/17 - 3s - 149ms/step - loss: 3.8275e-05 - mse: 3.8275e-05\n\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 180ms/step\nRMSE is: 39.6986532261871\nR2 is：0.9530147627049635\nMAE is：23.58051643192488\nWMAPE is: 0.0879134575784033\ntotal training time(s): 721.2224247455597\n","output_type":"stream"}]}]}
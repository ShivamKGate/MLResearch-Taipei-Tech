{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8408121,"sourceType":"datasetVersion","datasetId":5003711}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nfrom math import sqrt\nimport csv\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-24T11:26:09.561902Z","iopub.execute_input":"2024-05-24T11:26:09.562635Z","iopub.status.idle":"2024-05-24T11:26:09.566832Z","shell.execute_reply.started":"2024-05-24T11:26:09.562603Z","shell.execute_reply":"2024-05-24T11:26:09.565823Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:26:11.480605Z","iopub.execute_input":"2024-05-24T11:26:11.481309Z","iopub.status.idle":"2024-05-24T11:26:11.493863Z","shell.execute_reply.started":"2024-05-24T11:26:11.481267Z","shell.execute_reply":"2024-05-24T11:26:11.492757Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"/kaggle/input/adjacency.csv\n/kaggle/input/testresult/10-RMSE_ALL.txt\n/kaggle/input/testresult/10-Y_test_original.csv\n/kaggle/input/testresult/10-model-with-graph.h5\n/kaggle/input/testresult/10-MAE_ALL.txt\n/kaggle/input/testresult/10-model-with-graph.keras\n/kaggle/input/testresult/10-WMAPE_ALL.txt\n/kaggle/input/testresult/10-R2_ALL.txt\n/kaggle/input/testresult/10-predictions.csv\n/kaggle/input/testresult/10-Average_train_time.txt\n/kaggle/input/data/inflowdata/in_30min.csv\n/kaggle/input/data/inflowdata/in_15min.csv\n/kaggle/input/data/inflowdata/in_10min.csv\n/kaggle/input/data/outflowdata/out_10min.csv\n/kaggle/input/data/outflowdata/out_15min.csv\n/kaggle/input/data/outflowdata/out_30min.csv\n/kaggle/input/data/meteorology/30 min after normolization.csv\n/kaggle/input/data/meteorology/10 min after normolization.csv\n/kaggle/input/data/meteorology/15 min after normolization.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef Get_All_Data(TG,time_lag,TG_in_one_day,forecast_day_number,TG_in_one_week):\n\t#deal with inflow data 处理进站数据\n\tmetro_enter = []\n    \n\twith open('/kaggle/input/dataset2/data/inflowdata/in_'+str(TG)+'min.csv') as f:\n\t\tdata = csv.reader(f, delimiter=\",\")\n\t\tfor line in data:\n\t\t\tline=[int(x) for x in line]\n\t\t\tmetro_enter.append(line)\n\n\tdef get_train_data_enter(data,time_lag,TG_in_one_day,forecast_day_number,TG_in_one_week):\n\t\tdata = np.array(data)\n\t\tdata2 = np.zeros((data.shape[0], data.shape[1]))\n\t\ta = np.max(data)\n\t\tb = np.min(data)\n\t\tfor i in range(len(data)):\n\t\t\tfor j in range(len(data[0])):\n\t\t\t\tdata2[i, j] = round((data[i, j]-b)/(a-b), 5)\n\t\t#不包括第一周和最后一周的数据\n\t\t#not include the first week and the last week among the five weeks\n\t\tX_train_1 = [[] for i in range(TG_in_one_week, len(data2[0]) - time_lag+1 - TG_in_one_day*forecast_day_number)]\n\t\tY_train = []\n\t\tfor index in range(TG_in_one_week, len(data2[0]) - time_lag+1 - TG_in_one_day*forecast_day_number):\n\t\t\tfor i in range(276):\n\t\t\t\ttemp=data2[i,index-TG_in_one_week: index + time_lag-1-TG_in_one_week].tolist()\n\t\t\t\ttemp.extend(data2[i,index-TG_in_one_day: index + time_lag-1-TG_in_one_day])\n\t\t\t\ttemp.extend(data2[i,index: index + time_lag-1])\n\t\t\t\tX_train_1[index-TG_in_one_week].append(temp)\n\t\t\tY_train.append(data2[:,index + time_lag-1])\n\t\tX_train_1,Y_train = np.array(X_train_1), np.array(Y_train)\n\t\tprint(X_train_1.shape,Y_train.shape)\n\n\t\tX_test_1 = [[] for i in range(len(data2[0]) - TG_in_one_day*forecast_day_number,len(data2[0])-time_lag+1)]\n\t\tY_test = []\n\t\tfor index in range(len(data2[0]) - TG_in_one_day*forecast_day_number,len(data2[0])-time_lag+1):\n\t\t\tfor i in range(276):\n\t\t\t\ttemp = data2[i, index-TG_in_one_week: index + time_lag-1-TG_in_one_week].tolist()\n\t\t\t\ttemp.extend(data2[i, index-TG_in_one_day: index + time_lag-1-TG_in_one_day])\n\t\t\t\ttemp.extend(data2[i, index: index + time_lag-1])\n\t\t\t\tX_test_1[index-(len(data2[0]) - TG_in_one_day*forecast_day_number)].append(temp)\n\t\t\tY_test.append(data2[:, index + time_lag-1])\n\t\tX_test_1,Y_test = np.array(X_test_1), np.array(Y_test)\n\t\tprint(X_test_1.shape, Y_test.shape)\n\n\t\tY_test_original = []\n\t\tfor index in range(len(data[0]) - TG_in_one_day*forecast_day_number,len(data[0])-time_lag+1):\n\t\t\tY_test_original.append(data[:, index + time_lag-1])\n\t\tY_test_original = np.array(Y_test_original)\n\n\t\tprint(Y_test_original.shape)\n\n\t\treturn X_train_1,Y_train,X_test_1,Y_test,Y_test_original,a,b\n\n\t#获取训练集和测试集，Y_test_original为没有scale之前的原始测试集，评估精度用，a,b分别为最大值和最小值\n\t#Get the training dataset and the test dataset, Y_test_original is the original test data before scaling, which can be used for evaluation.\n\t#a and b as the maximum and minimum values, respectively.\n\tX_train_1,Y_train,X_test_1,Y_test,Y_test_original,a,b=get_train_data_enter(metro_enter,time_lag,TG_in_one_day,forecast_day_number,TG_in_one_week)\n\tprint(a,b)\n\n\t#deal with outflow data. Similar with the inflow data while not including the testing data for outflow\n\t#处理出站数据\n\tmetro_exit = []\n\twith open('/kaggle/input/dataset2/data/outflowdata/out_'+str(TG)+'min.csv') as f:\n\t\tdata = csv.reader(f, delimiter=\",\")\n\t\tfor line in data:\n\t\t\tline = [int(x) for x in line]\n\t\t\tmetro_exit.append(line)\n\n\tdef get_train_data_exit(data,time_lag,TG_in_one_day,forecast_day_number,TG_in_one_week):\n\t\tdata = np.array(data)\n\t\tdata2 = np.zeros((data.shape[0], data.shape[1]))\n\t\ta = np.max(data)\n\t\tb = np.min(data)\n\t\tfor i in range(len(data)):\n\t\t\tfor j in range(len(data[0])):\n\t\t\t\tdata2[i, j]=round((data[i, j]-b)/(a-b), 5)\n\t\tX_train_1 = [[] for i in range(TG_in_one_week, len(data2[0]) - time_lag+1 - TG_in_one_day*forecast_day_number)]\n\t\tfor index in range(TG_in_one_week, len(data2[0]) - time_lag+1 - TG_in_one_day*forecast_day_number):\n\t\t\tfor i in range(276):\n\t\t\t\ttemp=data2[i, index-TG_in_one_week: index + time_lag-1-TG_in_one_week].tolist()\n\t\t\t\ttemp.extend(data2[i, index-TG_in_one_day: index + time_lag-1-TG_in_one_day])\n\t\t\t\ttemp.extend(data2[i, index: index + time_lag-1])\n\t\t\t\tX_train_1[index-TG_in_one_week].append(temp)\n\t\tX_train_1 = np.array(X_train_1)\n\t\tprint(X_train_1.shape)\n\n\t\tX_test_1 = [[] for i in range(len(data2[0]) - TG_in_one_day*forecast_day_number, len(data2[0])-time_lag+1)]\n\t\tfor index in range(len(data2[0]) - TG_in_one_day*forecast_day_number, len(data2[0])-time_lag+1):\n\t\t\tfor i in range(276):\n\t\t\t\ttemp = data2[i,index-TG_in_one_week: index + time_lag-1-TG_in_one_week].tolist()\n\t\t\t\ttemp.extend(data2[i, index-TG_in_one_day: index + time_lag-1-TG_in_one_day])\n\t\t\t\ttemp.extend(data2[i, index: index + time_lag-1])\n\t\t\t\tX_test_1[index-(len(data2[0]) - TG_in_one_day*forecast_day_number)].append(temp)\n\t\tX_test_1 = np.array(X_test_1)\n\t\tprint(X_test_1.shape)\n\t\treturn X_train_1, X_test_1\n\n\tX_train_2, X_test_2 = get_train_data_exit(metro_exit, time_lag, TG_in_one_day, forecast_day_number, TG_in_one_week)\n\n\t#deal with graph data. involve the adjacency matrix 处理graph图数据，邻接矩阵信息\n\tadjacency = []\n\twith open('/kaggle/input/dataset2/adjacency.csv') as f:\n\t\tdata = csv.reader(f, delimiter=\",\")\n\t\tfor line in data:\n\t\t\tline = [float(x) for x in line]\n\t\t\tadjacency.append(line)\n\tadjacency = np.array(adjacency)\n\t# use adjacency matrix to calculate D_hat**-1/2 * A_hat *D_hat**-1/2\n\tI = np.matrix(np.eye(276))\n\tA_hat = adjacency+I\n\tD_hat = np.array(np.sum(A_hat, axis=0))[0]\n\tD_hat_sqrt = [sqrt(x) for x in D_hat]\n\tD_hat_sqrt = np.array(np.diag(D_hat_sqrt))\n\tD_hat_sqrtm_inv = np.linalg.inv(D_hat_sqrt)# get the D_hat**-1/2 (开方后求逆即为矩阵的-1/2次方)\n\t#D_A_final = D_hat**-1/2 * A_hat *D_hat**-1/2\n\tD_A_final = np.dot(D_hat_sqrtm_inv, A_hat)\n\tD_A_final = np.dot(D_A_final, D_hat_sqrtm_inv)\n\tprint(D_A_final.shape)\n\tdef get_train_data_graph(data,D_A_final,time_lag,TG_in_one_day,forecast_day_number,TG_in_one_week,):\n\t\tdata = np.array(data)\n\t\tdata2 = np.zeros((data.shape[0], data.shape[1]))\n\t\ta = np.max(data)\n\t\tb = np.min(data)\n\t\tfor i in range(len(data)):\n\t\t\tfor j in range(len(data[0])):\n\t\t\t\tdata2[i,j]=round((data[i,j]-b)/(a-b),5)\n\t\tX_train_1 = [[] for i in range(TG_in_one_week, len(data2[0]) - time_lag+1 - TG_in_one_day*forecast_day_number)]\n\t\tfor index in range(TG_in_one_week, len(data2[0]) - time_lag+1 - TG_in_one_day*forecast_day_number):\n\t\t\tfor i in range(276):\n\t\t\t\ttemp=data2[i,index: index + time_lag-1]\n\t\t\t\tX_train_1[index-TG_in_one_week].append(temp)\n\t\t\tX_train_1[index-TG_in_one_week] = np.dot(D_A_final, X_train_1[index-TG_in_one_week])\n\t\tX_train_1= np.array(X_train_1)\n\t\tprint(X_train_1.shape)\n\n\t\tX_test_1 = [[] for i in range(len(data2[0]) - TG_in_one_day*forecast_day_number,len(data2[0])-time_lag+1)]\n\t\tfor index in range(len(data2[0]) - TG_in_one_day*forecast_day_number,len(data2[0])-time_lag+1):\n\t\t\tfor i in range(276):\n\t\t\t\ttemp = data2[i,index: index + time_lag-1]\n\t\t\t\tX_test_1[index-(len(data2[0]) - TG_in_one_day*forecast_day_number)].append(temp)\n\t\t\tX_test_1[index-(len(data2[0]) - TG_in_one_day*forecast_day_number)] = np.dot(D_A_final, X_test_1[index-(len(data2[0]) - TG_in_one_day*forecast_day_number)])\n\t\tX_test_1 = np.array(X_test_1)\n\t\tprint(X_test_1.shape)\n\n\t\treturn X_train_1,X_test_1\n\n\tX_train_3, X_test_3 = get_train_data_graph(metro_enter, D_A_final, time_lag, TG_in_one_day, forecast_day_number, TG_in_one_week)\n\n\t#deal with meteorology data including the weather and PM data 处理11个指标的天气数据\n\tWeather = []\n\twith open('/kaggle/input/dataset2/data/meteorology/'+str(TG)+' min after normolization.csv') as f:\n\t\tdata = csv.reader(f, delimiter=\",\")\n\t\tfor line in data:\n\t\t\tline = [float(x) for x in line]\n\t\t\tWeather.append(line)\n\n\tdef get_train_data_weather_PM(data, time_lag, TG_in_one_day, forecast_day_number, TG_in_one_week,):\n\t\tdata = np.array(data)\n\t\t#不包括第一周和最后一周\n\t\tX_train_1 = [[] for i in range(TG_in_one_week, len(data[0]) - time_lag+1 - TG_in_one_day*forecast_day_number)]\n\t\tfor index in range(TG_in_one_week, len(data[0]) - time_lag+1 - TG_in_one_day*forecast_day_number):\n\t\t\tfor i in range(len(data)):\n\t\t\t\t#For meteorology data，we only consider today's data, namely recent pattern. 天气数据只考虑当天的\n\t\t\t\tX_train_1[index-TG_in_one_week].append(data[i,index: index + time_lag-1])\n\t\tX_train_1 = np.array(X_train_1)\n\t\tprint(X_train_1.shape)\n\n\t\tX_test_1 = [[] for i in range(len(data[0]) - TG_in_one_day*forecast_day_number, len(data[0])-time_lag+1)]\n\t\tfor index in range(len(data[0]) - TG_in_one_day*forecast_day_number, len(data[0])-time_lag+1):\n\t\t\tfor i in range(len(data)):\n\t\t\t\tX_test_1[index-(len(data[0]) - TG_in_one_day*forecast_day_number)].append(data[i, index: index + time_lag-1])\n\t\tX_test_1 = np.array(X_test_1)\n\t\tprint(X_test_1.shape)\n\t\treturn X_train_1,X_test_1\n\n\tX_train_4, X_test_4 = get_train_data_weather_PM(Weather, time_lag, TG_in_one_day, forecast_day_number, TG_in_one_week)\n\n\treturn X_train_1, Y_train, X_test_1, Y_test, Y_test_original, a, b, X_train_2, X_test_2, X_train_3, X_test_3, X_train_4, X_test_4\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom math import sqrt\nimport numpy as np\n\n#define weighted_mean_absolute_percentage_error and other eveluation metrics定义平均绝对百分比误差和评价函数\n# The shape of the two matrixs are all n*276 where 276 is the station numbers 两个矩阵都是n行276列\n\ndef weighted_mean_absolute_percentage_error(Y_true, Y_pred):\n\ttotal_sum=np.sum(Y_true)\n\taverage=[]\n\tfor i in range(len(Y_true)):\n\t\tfor j in range(len(Y_true[0])):\n\t\t\tif Y_true[i][j]>0:\n\t\t\t\t#加权 weighted\n\t\t\t\ttemp=(Y_true[i][j]/total_sum)*np.abs((Y_true[i][j] - Y_pred[i][j]) / Y_true[i][j])\n\t\t\t\taverage.append(temp)\n\treturn np.sum(average)\n\ndef evaluate_performance(Y_test_original,predictions):\n\tRMSE = sqrt(mean_squared_error(Y_test_original, predictions))\n\tprint('RMSE is: '+str(RMSE))\n\tR2 = r2_score(Y_test_original, predictions)\n\tprint(\"R2 is：\"+str(R2))\n\tMAE = mean_absolute_error(Y_test_original, predictions)\n\tprint(\"MAE is：\"+str(MAE))\n\tWMAPE = weighted_mean_absolute_percentage_error(Y_test_original, predictions)\n\tprint(\"WMAPE is: \"+str(WMAPE))\n\treturn RMSE, R2, MAE, WMAPE\n\nfrom numpy.random import seed\nseed(1)\nimport tensorflow as tf\ntf.random.set_seed(2)\nimport numpy as np\nnp.set_printoptions(threshold=np.inf)\nimport time\nimport os\n\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\ntf.keras.backend.set_image_data_format('channels_last')\n\n# Import layers, models, and utilities\nfrom tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, LSTM, Reshape, Permute, multiply, BatchNormalization, Activation\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam\n\n\n# os.chdir('D:/论文2/upload to GitHub/')\nos.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin' #used for visualizing the model\n\nglobal_start_time = time.time()\n\ndef Unit(x, filters, pool=False):\n\tres = x\n\tif pool:\n\t\tx = MaxPooling2D(pool_size=(2, 2), padding=\"same\")(x)\n\t\tres = Conv2D(filters=filters, kernel_size=[1, 1], strides=(2, 2), padding=\"same\")(res)\n\tout = BatchNormalization()(x)\n\tout = Activation(\"relu\")(out)\n\tout = Conv2D(filters=filters, kernel_size=[3, 3], strides=[1, 1], padding=\"same\")(out)\n\n\tout = BatchNormalization()(out)\n\tout = Activation(\"relu\")(out)\n\tout = Conv2D(filters=filters, kernel_size=[3, 3], strides=[1, 1], padding=\"same\")(out)\n\n\tout = tf.keras.layers.add([res, out])\n\n\treturn out\n\ndef attention_3d_block(inputs,timesteps): #input_dim = int(inputs.shape[2])\n    a = Permute((2, 1))(inputs)\n    a = Dense(timesteps, activation='linear')(a)\n    a_probs = Permute((2, 1))(a)\n    #output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n    output_attention_mul = multiply([inputs, a_probs])\n    return output_attention_mul\n","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:26:12.830268Z","iopub.execute_input":"2024-05-24T11:26:12.830622Z","iopub.status.idle":"2024-05-24T11:26:12.894405Z","shell.execute_reply.started":"2024-05-24T11:26:12.830594Z","shell.execute_reply":"2024-05-24T11:26:12.893298Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\n# Define the model\ndef multi_input_model(time_lag):\n    \"\"\"build multi input model构建多输入模型\"\"\"\n    input1_ = Input(shape=(276, time_lag-1, 3), name='input1')\n    input2_ = Input(shape=(276, time_lag-1, 3), name='input2')\n    input3_ = Input(shape=(276, time_lag-1, 1), name='input3')\n    input4_ = Input(shape=(11, time_lag-1, 1), name='input4')\n    #first input\n    x1 = Conv2D(filters=32, kernel_size=[3, 3], strides=[1, 1], padding=\"same\")(input1_)\n    x1 = Unit(x1, 32)\n    x1 = Unit(x1, 64, pool=True)\n    x1 = Flatten()(x1)\n    x1 = Dense(276)(x1)\n\n    # second input\n    x2 = Conv2D(filters=32, kernel_size=[3, 3], strides=[1, 1], padding=\"same\")(input2_)\n    x2 = Unit(x2, 32)\n    x2 = Unit(x2, 64, pool=True)\n    x2 = Flatten()(x2)\n    x2 = Dense(276)(x2)\n\n    # third input\n    x3 = Conv2D(filters=32, kernel_size=[3, 3], strides=[1, 1], padding=\"same\")(input3_)\n    x3 = Unit(x3, 32)\n    x3 = Unit(x3, 64, pool=True)\n    x3 = Flatten()(x3)\n    x3 = Dense(276)(x3)\n\n    # fourth input\n    x4 = Flatten()(input4_)\n    x4 = Dense(276)(x4)\n    x4 = Reshape(target_shape=(276, 1))(x4)\n    x4 = LSTM(128, return_sequences=True, input_shape=(276, 1))(x4)\n    x4 = LSTM(276, return_sequences=False)(x4)\n    x4 = Dense(276)(x4)\n\n    out = tf.keras.layers.add([x1, x2, x3, x4])\n    out = Reshape(target_shape=(276, 1))(out)\n    out = LSTM(128, return_sequences=True,input_shape=(276, 1))(out)\n    out = attention_3d_block(out, 276)#shape of the output is（276，128）\n    out = Flatten()(out)\n    out = Dense(276)(out)\n\n    model = Model(inputs=[input1_, input2_, input3_,input4_], outputs=[out]) #[input1_, input2_, input3_]\n    return model\n\ndef build_model(X_train_1,X_train_2,X_train_3,X_train_4,Y_train,X_test_1,X_test_2,X_test_3,X_test_4,Y_test, Y_test_original,batch_size,epochs,a,time_lag):\n\tX_train_1 = X_train_1.reshape(X_train_1.shape[0],  276, time_lag-1, 3)\n\tX_train_2 = X_train_2.reshape(X_train_2.shape[0],  276, time_lag-1, 3)\n\tX_train_3 = X_train_3.reshape(X_train_3.shape[0],  276, time_lag-1, 1)\n\tX_train_4 = X_train_4.reshape(X_train_4.shape[0],  11, time_lag-1, 1)\n\tY_train = Y_train.reshape(Y_train.shape[0], 276)\n\n\tX_test_1 = X_test_1.reshape(X_test_1.shape[0],  276, time_lag-1, 3)\n\tX_test_2 = X_test_2.reshape(X_test_2.shape[0],  276, time_lag-1, 3)\n\tX_test_3 = X_test_3.reshape(X_test_3.shape[0],  276, time_lag-1, 1)\n\tX_test_4 = X_test_4.reshape(X_test_4.shape[0],  11, time_lag-1, 1)\n\tY_test = Y_test.reshape(Y_test.shape[0], 276)\n\n\tif epochs == 55:\n\t\tmodel = multi_input_model(time_lag)\n\t\tmodel.compile(optimizer=Adam(), loss='mse', metrics=['mse'])\n\t\tmodel.fit([X_train_1, X_train_2, X_train_3, X_train_4], Y_train, batch_size=batch_size, epochs=epochs, verbose=2, shuffle=False)#, validation_split=0.05\n\t\toutput = model.predict([X_test_1, X_test_2, X_test_3, X_test_4], batch_size=batch_size)\n\telse:\n\t\t# Load the previously saved model and continue training\n\t\tmodel_path = f'/kaggle/working/testresult/{epochs - 10}/10-model-with-graph.keras'\n\t\tif os.path.exists(model_path):\n\t\t\tmodel = load_model(model_path)\n\t\t\tmodel.fit([X_train_1, X_train_2, X_train_3, X_train_4], Y_train, batch_size=batch_size, epochs=10,\n\t\t\t\t\t  verbose=2, shuffle=False)\n\t\t\toutput = model.predict([X_test_1, X_test_2, X_test_3, X_test_4], batch_size=batch_size)\n\t\telse:\n\t\t\traise FileNotFoundError(f\"Model file not found: {model_path}\")\n    \n    \n\t#rescale the output of this model将输出进行反归一化\n\tpredictions = np.zeros((output.shape[0], output.shape[1]))\n\tfor i in range(len(predictions)):\n\t\tfor j in range(len(predictions[0])):\n\t\t\tpredictions[i, j] = round(output[i, j]*a, 0)\n\t\t\tif predictions[i, j] < 0:\n\t\t\t\tpredictions[i, j] = 0\n\n\tRMSE,R2,MAE,WMAPE=evaluate_performance(Y_test_original,predictions)\n\t#visualize the model structure\n\t#print(model.summary())\n\tplot_model(model, to_file='model.png', show_shapes=True)\n\n\treturn model,Y_test_original,predictions,RMSE,R2,MAE,WMAPE\n\nimport os\nimport time\nimport numpy as np\n\ndef Save_Data(path, model, Y_test_original, predictions, RMSE, R2, MAE, WMAPE, Run_epoch, global_start_time):\n    directory = os.path.join(path, str(Run_epoch))\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n        \n    model.save(os.path.join(directory, '10-model-with-graph.keras'))\n    np.savetxt(os.path.join(directory, '10-RMSE_ALL.txt'), [RMSE])\n    np.savetxt(os.path.join(directory, '10-R2_ALL.txt'), [R2])\n    np.savetxt(os.path.join(directory, '10-MAE_ALL.txt'), [MAE])\n    np.savetxt(os.path.join(directory, '10-WMAPE_ALL.txt'), [WMAPE])\n    np.savetxt(os.path.join(directory, '10-predictions.csv'), predictions, delimiter=\",\")\n    np.savetxt(os.path.join(directory, '10-Y_test_original.csv'), Y_test_original, delimiter=\",\")\n    \n    duration_time = time.time() - global_start_time\n    np.savetxt(os.path.join(directory, '10-Average_train_time.txt'), [duration_time])\n    \n    print('total training time(s):', duration_time)\n\n# Define the path to save the results\noutput_path = \"/kaggle/working/testresult/\"\n\n# Assuming Get_All_Data and build_model functions are defined elsewhere\nX_train_1, Y_train, X_test_1, Y_test, Y_test_original, a, b, X_train_2, X_test_2, X_train_3, X_test_3, X_train_4, X_test_4 = Get_All_Data(TG=15, time_lag=6, TG_in_one_day=72, forecast_day_number=5, TG_in_one_week=360)\n\nRun_epoch = 55  # first training 50 epoch, and then add 10 epoch every time 初始训练epoch，以后每次加10，运行15次\nglobal_start_time = time.time()\n\nfor i in range(15):\n    model, Y_test_original, predictions, RMSE, R2, MAE, WMAPE = build_model(X_train_1, X_train_2, X_train_3, X_train_4, Y_train, X_test_1, X_test_2, X_test_3, X_test_4, Y_test,Y_test_original, batch_size=64, epochs=Run_epoch, a=a, time_lag=6)\n    Save_Data(output_path, model, Y_test_original, predictions, RMSE, R2, MAE, WMAPE, Run_epoch, global_start_time)\n    Run_epoch += 10\n\n\n#For Get_All_Data, change parameters referring to this: TG=15, time_lag=6, TG_in_one_day=72, forecast_day_number=5, TG_in_one_week=360\n#10min:10,6,108,5,540,eopch=200\n#15min:15,6,72,5,360 eopch=140\n#30min:30,6,36,5,180 eopch=200\n#60min:60,6,18,5,90 eopch=235","metadata":{"execution":{"iopub.status.busy":"2024-05-24T11:26:14.225981Z","iopub.execute_input":"2024-05-24T11:26:14.226403Z","iopub.status.idle":"2024-05-24T11:26:14.486405Z","shell.execute_reply.started":"2024-05-24T11:26:14.226366Z","shell.execute_reply":"2024-05-24T11:26:14.485183Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 118\u001b[0m\n\u001b[1;32m    115\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/testresult/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Assuming Get_All_Data and build_model functions are defined elsewhere\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m X_train_1, Y_train, X_test_1, Y_test, Y_test_original, a, b, X_train_2, X_test_2, X_train_3, X_test_3, X_train_4, X_test_4 \u001b[38;5;241m=\u001b[39m \u001b[43mGet_All_Data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTG\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_lag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTG_in_one_day\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m72\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforecast_day_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTG_in_one_week\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m360\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m Run_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m55\u001b[39m  \u001b[38;5;66;03m# first training 50 epoch, and then add 10 epoch every time 初始训练epoch，以后每次加10，运行15次\u001b[39;00m\n\u001b[1;32m    121\u001b[0m global_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n","Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36mGet_All_Data\u001b[0;34m(TG, time_lag, TG_in_one_day, forecast_day_number, TG_in_one_week)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGet_All_Data\u001b[39m(TG,time_lag,TG_in_one_day,forecast_day_number,TG_in_one_week):\n\u001b[1;32m      2\u001b[0m \t\u001b[38;5;66;03m#deal with inflow data 处理进站数据\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \tmetro_enter \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m \t\u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/dataset2/data/inflowdata/in_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTG\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      6\u001b[0m \t\tdata \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(f, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \t\t\u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/dataset2/data/inflowdata/in_15min.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/input/dataset2/data/inflowdata/in_15min.csv'","output_type":"error"}]},{"cell_type":"code","source":"/kaggle/input/dataset2/data/inflowdata/in_15min.csv","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}